---
layout: default
description: Collecting info to escalate to the carrier-runtime squad
title: ibm-worker-autorecovery - Collecting info before escalation
service: ibm-worker-autorecovery
runbook-name: "ibm-worker-autorecovery -  Collecting info before escalation"
tags: alchemy, armada, bootstrap, autorecovery,
link: /armada/autorecovery-information-collection.html
type: Informational
parent: Armada
grand_parent: Armada Runbooks
---

Informational
{: .label }

## Overview

The auto-recovery problem you are escalating to the carrier-runtime team likely has a worker associated with it. When escalating to the carrier-runtime squad you will need to collect the following information about your problem for the team to analyze. The workerID will be specified in the pager duty incident

## Collecting Logs For auto-recovery
Get the application logs from the ibm-worker-recovery pod that services the worker you are having problems with<br><br>
Auto-recovery uses a consistent hashing algorithm, so a worker will belong to one recovery pod at a time.

~~~~~
export WORKER_ID="workerID" (example: WORKER_ID="kube-dal10-cr2ceff3fc1e214d66a1400ca632c61e59-w131")

export LOGS=$(for i in `kubectl -n kube-system get pods | grep "worker-recovery" | awk '{print $1}'`; \
do kubectl -n kube-system logs --tail=500 $i | grep $WORKER_ID; done)

IFS=$'\n'; set -f; for i in $LOGS; do echo "Event date: $(date -d @$(echo $i| jq '.ts'))"; echo $i | jq .; done
~~~~~
If there are no entries, increase the tail size.


## Detailed Information
To see the config-maps generated by ibm-worker-recovery

~~~~~
kubectl get cm -n kube-system
~~~~~

The config-map that controls the checks done by auto-recovery, leading to corrective action, is called `ibm-worker-recovery-checks`
To see the checks enabled for autorecovery

~~~~~
kubectl get cm -n kube-system ibm-worker-recovery-checks -o yaml
~~~~~

To see the correctiveaction generated by ibm-worker-recovery

~~~~~
kubectl get correctiveaction -n kube-system
~~~~~

There are two important correctiveaction show the status of corrective actions taken by auto-recovery
The last-fix correctiveaction keeps the time of the last corrective action taken for a worker. The format is
~~~~~
<nodeName>-<index>-last-fix
Examples:
10.177.155.188-2-last-fix
10.177.155.19-2-last-fix
10.177.155.46-2-last-fix
~~~~~

The last-fix correctiveaction act as a cool-off for a worker. If the current time - timestamp in last-fix is less than the cool-off value set in the checks config-map, auto-recovery will not take another corrective action on the worker.
The last-fix correctiveaction also holds the information of the last corrective action taken, and the providerID of the worker the action was taken against.

~~~~~
Example:
kubectl get correctiveaction -n kube-system -o yaml 10.131.16.89-8-last-fix

action_request_id: ""
apiVersion: workerrecovery.stable/v1
carrier_action_status: ""
check_name: worker-config.json
data_ip: 10.131.16.89
data_unique_identifier: "8"
kind: CorrectiveAction
last_fix_timestamp: 2021-02-28T13:48:49+0000
metadata:
  creationTimestamp: "2021-02-28T13:48:49Z"
  generation: 1
  labels:
    labelSelector: last-fix
  managedFields:
  - apiVersion: workerrecovery.stable/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:action_request_id: {}
      f:carrier_action_status: {}
      f:check_name: {}
      f:data_ip: {}
      f:data_unique_identifier: {}
      f:last_fix_timestamp: {}
      f:metadata:
        f:labels:
          .: {}
          f:labelSelector: {}
      f:requested_action: {}
      f:requested_provider_id: {}
    manager: ibm-worker-recovery
    operation: Update
    time: "2021-02-28T13:48:49Z"
  name: 10.131.16.89-8-last-fix
  namespace: kube-system
  resourceVersion: "2586893904"
  selfLink: /apis/workerrecovery.stable/v1/namespaces/kube-system/correctiveactions/10.131.16.89-8-last-fix
  uid: 65b15990-c80d-4b76-bf6c-b110a97eecc7
requested_action: CARRIERRELOAD
requested_provider_id: ibm://///carrier5/dev-mex01-carrier5-worker-08.alchemy.ibm.com
~~~~~

The action-label correctiveaction is created when a corrective action is currently in progress for a worker. It contains the same information as the last-fix correctiveaction, but signifies an active corrective action

## Other Features

ibm-worker-autorecovery will only take corrective action against one worker at a time. If there is a current `action-label` correctiveaction all other workers will be blocked from auto-recovering until the current worker is back in a healthy state

Autorecovery events are also posted to the deployment file

~~~~~
kubectl describe deployment-n kube-system ibm-worker-recovery
~~~~~

Autorecovery will delete the active action-label correctiveaction after the associated worker shows a healthy status for a designated poll period. This poll period will be in the config-map for the service

~~~~~
kubectl get cm -n kube-system ibm-worker-recovery-config -o yaml

Example:
apiVersion: v1
data:
  nurse.toml: |
    [Partitioner]
    unique_label_value = "nurse"
    unique_label_key = "nursekey"
    pod_id = ""
    namespace = "kube-system"
    peer_cache_ttl_seconds = 10

    [General]
    check_information_direction = "/tmp/checkdirectory"
    max_workers = 50
    kube_api_timeout_ms = 10000
    activity_tracker_queue_length = 40
    kube_api_timeout_seconds = 10
    node_cache_ttl_seconds = 10
    armada_api_timeout_seconds = 60
    node_corrector_check_minutes = 1
    node_ready_poll_minutes = 5
    checks_information_timeout_seconds = 30
kind: ConfigMap
~~~~~

## Escalation
Gather all relevant information and create an issue here [here](https://github.ibm.com/alchemy-containers/armada-carrier/issues/new). If paging, include the link to the issue that has all the information from above above.

If a CIE has been raised and you need assistance, please engage the development squad using the [{{ site.data.teams.armada-carrier.escalate.name }}]({{ site.data.teams.armada-carrier.escalate.link }}) pagerduty escalation policy.

If this is not a CIE, you can reach out using the [{{ site.data.teams.armada-carrier.comm.name }}]({{ site.data.teams.armada-carrier.comm.link }}) Slack channel or create a issue in the [armada-carrier](https://github.ibm.com/alchemy-containers/armada-carrier/issues/new) Github repository for later follow-up.
